{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d06274e9",
   "metadata": {},
   "source": [
    "## Neural Networks Regression - Boston Housing Price prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c8264d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install libraries:\n",
    "# pip install keras\n",
    "# pip install tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31ca5f28",
   "metadata": {},
   "source": [
    "### Importing Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "80a19563",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9a9d2c0",
   "metadata": {},
   "source": [
    "### Reading Dataset and spliting into features and target variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8766e212",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = pandas.read_csv(\"./housing.data\", delim_whitespace=True, header=None)\n",
    "dataset = dataframe.values\n",
    "\n",
    "# spliting the dataset into features(X) and the output target variable - Price (Y)\n",
    "X = dataset[:,0:13]\n",
    "Y = dataset[:,13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0e25c8f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[6.3200e-03, 1.8000e+01, 2.3100e+00, ..., 1.5300e+01, 3.9690e+02,\n",
       "        4.9800e+00],\n",
       "       [2.7310e-02, 0.0000e+00, 7.0700e+00, ..., 1.7800e+01, 3.9690e+02,\n",
       "        9.1400e+00],\n",
       "       [2.7290e-02, 0.0000e+00, 7.0700e+00, ..., 1.7800e+01, 3.9283e+02,\n",
       "        4.0300e+00],\n",
       "       ...,\n",
       "       [6.0760e-02, 0.0000e+00, 1.1930e+01, ..., 2.1000e+01, 3.9690e+02,\n",
       "        5.6400e+00],\n",
       "       [1.0959e-01, 0.0000e+00, 1.1930e+01, ..., 2.1000e+01, 3.9345e+02,\n",
       "        6.4800e+00],\n",
       "       [4.7410e-02, 0.0000e+00, 1.1930e+01, ..., 2.1000e+01, 3.9690e+02,\n",
       "        7.8800e+00]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c374177",
   "metadata": {},
   "source": [
    "Below we define the function to create the baseline model to be evaluated. It is a simple model that has a single fully connected hidden layer with the same number of neurons as input attributes (13). The network uses good practices such as the rectifier activation function for the hidden layer. No activation function is used for the output layer because it is a regression problem and we are interested in predicting numerical values directly without transform.\n",
    "\n",
    "The efficient ADAM optimization algorithm is used and a mean squared error loss function is optimized. This will be the same metric that we will use to evaluate the performance of the model. It is a desirable metric because by taking the square root gives us an error value we can directly understand in the context of the problem (thousands of dollars)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f3fb8918",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define baseline model to evaluate Neural network\n",
    "def baseline_model():\n",
    "    # Creating a sequential object for adding sequences of layers\n",
    "    model = Sequential()\n",
    "    # Adding a single input layer with same number of neurons as the number of features (13)\n",
    "    # Activation function used 'relu' function \n",
    "    model.add(Dense(13, input_dim=13, kernel_initializer='normal', activation='relu'))\n",
    "    \n",
    "    # Being a Regression problem, there is no activation layer in the output layer\n",
    "    model.add(Dense(1, kernel_initializer='normal'))\n",
    "    \n",
    "    # Compile the model 'adam' optimiser and having a loss function as mean squared error\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "78e749cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "141e25b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating variables \n",
    "epochs = 100\n",
    "batch_size = 5\n",
    "\n",
    "# This method is used to perform 10 fold cross validation on the dataset and provide the results\n",
    "def validate_model(estimator):\n",
    "    # Using 10 fold cross validation and taking the average error\n",
    "    kfold = KFold(n_splits=10)\n",
    "    results = cross_val_score(estimator, X, Y, cv=kfold)\n",
    "    \n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ccd7cb7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a5719ef9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results: -32.77 (31.02) MSE\n"
     ]
    }
   ],
   "source": [
    "# evaluate Baseline model \n",
    "estimator = KerasRegressor(build_fn=baseline_model, epochs=epochs, batch_size=batch_size, verbose=0)\n",
    "results = validate_model(estimator)\n",
    "print(\"Results: %.2f (%.2f) MSE\" % (results.mean(), results.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85727110",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bfffbe9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b06ce89",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3b0653e",
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aef61f35",
   "metadata": {},
   "source": [
    "### Observation:\n",
    "\n",
    "The result reports the mean squared error including the average and standard deviation (average variance) across all 10 folds of the cross validation evaluation.\n",
    "\n",
    "Note: The mean squared error is negative because scikit-learn inverts so that the metric is maximized instead of minimized. You can ignore the sign of the result."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d195fe7",
   "metadata": {},
   "source": [
    "### Standardisation of dataset and re-evaluatin the baseline model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9fca03b",
   "metadata": {},
   "source": [
    "We can use scikit-learnâ€™s Pipeline framework to perform the standardization during the model evaluation process, within each fold of the cross validation. This ensures that there is no data leakage from each testset cross validation fold into the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f58ad573",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standardized: -22.19 (22.55) MSE\n"
     ]
    }
   ],
   "source": [
    "estimators = []\n",
    "# Standardizing the feautures\n",
    "estimators.append(('standardize', StandardScaler()))\n",
    "estimators.append(('mlp', KerasRegressor(build_fn=baseline_model, epochs=epochs, batch_size=batch_size, verbose=0)))\n",
    "pipeline = Pipeline(estimators)\n",
    "\n",
    "results = validate_model(pipeline)\n",
    "print(\"Standardized: %.2f (%.2f) MSE\" % (results.mean(), results.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba850c19",
   "metadata": {},
   "source": [
    "### Observation:\n",
    "\n",
    "Running the example provides an improved performance over the baseline model without standardized data, dropping the error.\n",
    "\n",
    "A further extension of this section would be to similarly apply a rescaling to the output variable such as normalizing it to the range of 0-1 and use a Sigmoid or similar activation function on the output layer to narrow output predictions to the same range."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cdc5c7d",
   "metadata": {},
   "source": [
    "## Tune The Neural Network Topology"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0eea209",
   "metadata": {},
   "source": [
    "### Evaluate a Deeper Network Topology\n",
    "\n",
    "One way to improve the performance a neural network is to add more layers. This might allow the model to extract and recombine higher order features embedded in the data.\n",
    "\n",
    "In this section we will evaluate the effect of adding one more hidden layer to the model. This is as easy as defining a new function that will create this deeper model, copied from our baseline model above. We can then insert a new line after the first hidden layer. In this case with about half the number of neurons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a9ade8cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the model\n",
    "def deeper_model():\n",
    "    # Creating a sequential object for adding sequences of layers\n",
    "    model = Sequential()\n",
    "    # Adding a single input layer with same number of neurons as the number of features (13)\n",
    "    # Activation function used 'relu' function \n",
    "    model.add(Dense(13, input_dim=13, kernel_initializer='normal', activation='relu'))\n",
    "    \n",
    "    # Adding a single hidden layer with 6 neurons and 'relu' as Activation function\n",
    "    model.add(Dense(6, kernel_initializer='normal', activation='relu'))\n",
    "    \n",
    "    # Being a Regression problem, there is no activation layer in the output layer\n",
    "    model.add(Dense(1, kernel_initializer='normal'))\n",
    "    \n",
    "    # Compile the model 'adam' optimiser and having a loss function as mean squared error\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3280acef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deeper Model: -21.83 (23.03) MSE\n"
     ]
    }
   ],
   "source": [
    "estimators = []\n",
    "# Standardizing the feautures\n",
    "estimators.append(('standardize', StandardScaler()))\n",
    "estimators.append(('mlp', KerasRegressor(build_fn=deeper_model, epochs=epochs, batch_size=batch_size, verbose=0)))\n",
    "pipeline = Pipeline(estimators)\n",
    "\n",
    "results = validate_model(pipeline)\n",
    "print(\"Deeper Model: %.2f (%.2f) MSE\" % (results.mean(), results.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8acb234d",
   "metadata": {},
   "source": [
    "### Observation:\n",
    "\n",
    "Running this model does show a further improvement in performance from 28 down to 24 thousand squared dollars."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e239770f",
   "metadata": {},
   "source": [
    "### Evaluate a Wider Network Topology\n",
    "\n",
    "Another approach to increasing the representational capability of the model is to create a wider network.\n",
    "\n",
    "In this section we evaluate the effect of keeping a shallow network architecture and nearly doubling the number of neurons in the one hidden layer.\n",
    "\n",
    "Again, all we need to do is define a new function that creates our neural network model. Here, we have increased the number of neurons in the hidden layer compared to the baseline model from 13 to 20."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9eddb9b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wider_model():\n",
    "    # Creating a sequential object for adding sequences of layers\n",
    "    model = Sequential()\n",
    "    # Adding a single input layer with increased number of neurons (say 20) and with the number of features (13)\n",
    "    # Activation function used 'relu' function \n",
    "    model.add(Dense(20, input_dim=13, kernel_initializer='normal', activation='relu'))\n",
    "    \n",
    "    # Being a Regression problem, there is no activation layer in the output layer\n",
    "    model.add(Dense(1, kernel_initializer='normal'))\n",
    "    \n",
    "    # Compile the model 'adam' optimiser and having a loss function as mean squared error\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dd3cfd86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wider Model: -21.98 (22.94) MSE\n"
     ]
    }
   ],
   "source": [
    "estimators = []\n",
    "# Standardizing the feautures\n",
    "estimators.append(('standardize', StandardScaler()))\n",
    "estimators.append(('mlp', KerasRegressor(build_fn=wider_model, epochs=epochs, batch_size=batch_size, verbose=0)))\n",
    "pipeline = Pipeline(estimators)\n",
    "results = validate_model(pipeline)\n",
    "print(\"Wider Model: %.2f (%.2f) MSE\" % (results.mean(), results.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eb3ed18",
   "metadata": {},
   "source": [
    "### Observation:\n",
    "\n",
    "Building the model does see a further drop in error to about 21 thousand squared dollars. This is not a bad result for this problem.\n",
    "\n",
    "It would have been hard to guess that a wider network would outperform a deeper network on this problem. The results demonstrate the importance of empirical testing when it comes to developing neural network models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf5d24b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a8401ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c40973",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b5d32c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7abcd780",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed7283ab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
